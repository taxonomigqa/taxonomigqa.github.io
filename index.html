<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Vision-language training improves the deployment of taxonomic knowledge in LMs for text-only tasks, rather than fundamentally altering the knowledge itself. Introducing TaxonomiGQA.">
  <meta property="og:title" content="Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It"/>
  <meta property="og:description" content="VL training enhances LM's ability to deploy taxonomic knowledge in text-only tasks, without fundamentally changing the knowledge itself. See our new TaxonomiGQA dataset."/>
  <meta property="og:url" content="https://your-github-username.github.io/your-repo-name/"/> <!-- UPDATE THIS URL -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/images/taxonomigqa_banner.png" /> <!-- RECOMMEND CREATING A BANNER IMAGE (e.g., from Figure 1 or key results) -->
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="VL Training: Deploying Taxonomic Knowledge, Not Altering It">
  <meta name="twitter:description" content="Our study shows VL training helps LMs deploy taxonomic knowledge in text-only tasks, linked to visual concept similarity. Introducing TaxonomiGQA.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="static/images/taxonomigqa_twitter_banner.png"> <!-- RECOMMEND CREATING A TWITTER BANNER IMAGE -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-and-Language Models, Language Models, Taxonomic Knowledge, Conceptual Knowledge, Text-only QA, GQA, TaxonomiGQA, VLM, LM, Grounding, Neural Networks, NeurIPS">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Vision-and-Language Training: Taxonomic Knowledge Deployment</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
             <!-- Title icon and text aligned -->
             <h1 class="title is-1 publication-title">
              <div class="is-flex is-align-items-center is-justify-content-center" style="display: flex;">
                <img src="static/images/taxonomicGQA-title-transparent.png" alt="TaxonomicGQA Icon" style="height: 3em; margin-right: 0.5em;">
                <span style="line-height: 1;">
                  Vision and Language Training Helps Deploy Taxonomic Knowledge<br>But Does Not Fundamentally Alter It
                </span>
              </div>
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yuluqinn.github.io/" target="_blank">Yulu Qin,</a><sup>1,*</sup></span>
                <span class="author-block">
                  <a href="https://dhevarghese.github.io" target="_blank">Dheeraj Varghese,</a><sup>2,*</sup></span>
                  <span class="author-block">
                    <a href="https://www.umu.se/en/staff/adam-dahlgren-lindstrom/" target="_blank">Adam Dahlgren Lindström,</a><sup>3</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://vu.nl/en/research/scientists/lucia-donatelli" target="_blank">Lucia Donatelli,</a><sup>4</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://kanishka.website/" target="_blank">Kanishka Misra,</a><sup>5,†</sup>
                  </span>  
                  <span class="author-block">
                    <a href="https://najoung.kim/" target="_blank">Najoung Kim</a><sup>1,†</sup>
                  </span>  
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Boston University, <sup>2</sup>University of Amsterdam, <sup>3</sup>Umeå Universitet, <sup>4</sup>Vrije Universiteit Amsterdam, <br><sup>5</sup>Toyota Technological Institute at Chicago</span>
                    <span class="eql-cntrb"><small><br><sup>*,†</sup> indicate equal contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Paper PDF link -->
                      <span class="link-block">
                        <a class="external-link button is-normal is-rounded is-dark" disabled title="Coming Soon!">
                          <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>

                      <!-- Dataset link -->
                      <span class="link-block">
                        <a class="external-link button is-normal is-rounded is-dark" href="https://huggingface.co/datasets/tin-lab/TaxonomiGQA" target="_blank" title="View Dataset">
                          <span class="icon">
                            <i class="fas fa-database"></i>
                          </span>
                          <span>Dataset</span>
                        </a>
                      </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a class="external-link button is-normal is-rounded is-dark" href="https://github.com/tinlaboratory/taxonomigqa" target="_blank" title="View Code on GitHub">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/approach_overview.png" alt="Overview of our experimental approach comparing VLMs and LMs on a text-only QA task derived from VQA." width="100%" height="100%">
      <p class="content is-italic has-text-centered mt-2">
        We convert visual scenes into rich text descriptions to create our <strong>TaxonomiGQA</strong> dataset. Both the Vision-Language Model (VLM) and its base Language Model (LM) are then evaluated on this purely text-based QA task, which includes hypernym substitutions. This allows for a controlled comparison of how vision training affects taxonomic understanding.
      </p>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Our Contributions -->
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Our Contributions</h2>
          <div class="content has-text-justified">
            <p>We investigate how vision-and-language training impacts the taxonomic knowledge of language models. To this end, we make the following contributions:</p>
            <ul>
              <li><strong>TaxonomiGQA:</strong> We introduce a new text-only question-answering dataset derived from GQA that isolates and tests taxonomic understanding. The dataset includes <strong>148,020 examples</strong>, covering <strong>126 hypernym chains</strong>, and emphasizes robust hierarchical knowledge using positive and multiple hard negative examples.</li>
              <li><strong>Systematic Evaluation of Minimal LM–VLM Pairs:</strong> We evaluate seven minimal LM-VLM pairs, where each VLM is built on top of its LM counterpart. In six out of seven pairs, VLMs consistently outperform LMs on this task.</li>
              <li><strong>Distinguishing Knowledge from Deployment:</strong> Using behavioral and representational analyses, we find that while the underlying taxonomic knowledge between VLMs and LMs are not significantly different, VLMs are substantially better at <em>deploying</em> that knowledge in task contexts.</li>
              <li><strong>Visual Similarity as a facilitator:</strong> We show preliminary evidence that the performance gains in VLMs can be explained by visual similarity and cohesion between concepts in the taxonomy, suggesting that visual grounding may particularly boost the deployment of taxonomic knowledge in cases where category members are largely similar to the larger category.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
</section>
<!-- End Our Contributions -->

<!-- TaxonomiGQA Dataset Section -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">TaxonomiGQA Dataset</h2>
          <div class="content has-text-justified">
            <p>We apply a three-step transformation to GQA to create TaxonomiGQA:</p>
            <ol>
              <li>
                <strong>Scene Descriptions:</strong> Convert scene graphs into textual descriptions of the scene programmatically using hand-crafted templates.
                <!-- <br><em>Example:</em> “There are 3 objects in the scene, including a road, a black bear, and a car. The bear is crossing the road.” -->
              </li>
              <li>
                <strong>Hypernym Substitution:</strong> Replace objects in questions with hypernyms sampled from their WordNet-derived hierarchy.
                <!-- <br><em>Example:</em> “Are there any <strong>mammals</strong>?” instead of “Are there any <strong>bears</strong>?” -->
              </li>
              <li>
                <strong>Negative Sampling:</strong> Create four hard negative samples per question by replacing the target term with a non-hypernym that is absent from the scene.
              </li>
            </ol>

            <div class="content has-text-centered my-5">
              <img src="static/images/scene-description.png" alt="Illustration of the three-step pipeline for creating the TaxonomiGQA dataset." width="100%" height="100%">
              <p class="is-size-7 mt-2">
                Our three-step pipeline for generating the TaxonomiGQA dataset. 
              </p>
            </div>
  
            <h3 class="title is-4">Dataset Summary</h3>
            <div class="columns is-multiline is-mobile">
              <div class="column is-half">🔹 <strong>Total Instances:</strong> 148,020</div>
              <div class="column is-half">🔹 <strong>Unique Scenes:</strong> 1,342</div>
              <div class="column is-half">🔹 <strong>Positive Samples:</strong> 29,604</div>
              <!-- <div class="column is-half">🔸 <em>Leaf-node:</em> 9,334</div>
              <div class="column is-half">🔸 <em>Hypernym-substituted:</em> 20,270</div> -->
              <div class="column is-half">🔹 <strong>Negative Samples:</strong> 118,416 (4 per positive)</div>
              <div class="column is-half">🔹 <strong>Hyponym-Hypernym Pairs:</strong> 276</div>
              <div class="column is-half">🔹 <strong>Hypernym Chains:</strong> 126</div>
              <div class="column is-half">🔹 <strong>Unique Hypernyms:</strong> 88</div>
              <div class="column is-half">🔹 <strong>Top-Level Categories:</strong> 24</div>
            </div>
            
          
            <p>More details on question filtering and negative sampling can be found in <strong>Appendix C</strong> of the paper.</p>
          </div>
        </div>
      </div>
    </div>
  </section>  
<!-- End TaxonomiGQA Dataset Section -->

<!-- Experiments & Key Findings -->
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experiments & Key Findings</h2>
          <div class="content has-text-justified">
            <p>
              We evaluated <strong>7 minimal pairs</strong> of LMs and their VLM counterparts, where each pair shares the same base language model.
            </p>
  
            <h3 class="title is-4">VLMs Consistently Outperform LMs on Text-Only QA</h3>
            <p>
              VLMs achieved higher scores than LMs on all three evaluation metrics:
              <em>Overall Accuracy</em>, <em>Conditional Accuracy</em>, and <em>Hierarchical Consistency</em>. This held across nearly all model pairs (with the exception of Vicuna vs. Llava-1.5), suggesting that VL training improves a model's ability to apply taxonomic knowledge.
            </p>
            <div class="content has-text-centered">
              <img src="static/images/detailed-VLM_LM_performance.png" alt="VLM LM Performance on TaxonomiGQA" width="100%" height="auto">
              <p class="is-size-7">Performance of VLM-LM model pairs on TaxonomiGQA and TAXOMPS. Points above the diagonal indicate VLM outperformance.</p>
            </div>
            <p>
              To explain these observations, we put forth two hypotheses:
            </p>
  
            <h3 class="title is-4">H1: VLMs' taxonomic knowledge aligns better with the reference taxonomy</h3>
            <p>
              To test whether VL training alters the underlying taxonomic knowledge itself, we introduced <strong>TAXOMPS</strong> (taxonomic minimal pairs), a QA dataset constructed on top of our taxonomy, which directly asks whether one concept is a kind of another (e.g., “Is a cat an animal?”) along with four minimal pair questions (e.g., "Is a cat a vegetable?"). Both LMs and VLMs performed similarly well, indicating that their <em>underlying</em> taxonomic knowledge remains largely unchanged by VL training. We further confirmed this through Representational Similarity Analysis (RSA) on top of hierarchically sensitive representations extracted from the LM and the VLM (using the method developed by <a href="https://arxiv.org/abs/2406.01506">Park et al., 2024</a>). We found no significant difference between these representations in any LM-VLM pair.
            </p>
  
            <h3 class="title is-4">H2: VL Training improves the deployment of taxonomic knowledge</h3>
            <p>
              While their underlying knowledge was found to be similar to that of LMs, VLMs show a clear advantage in how they <em>deploy</em> that knowledge in task contexts. Two key analyses performed on the Qwen 2.5 pair (most salient differences in main results) support this:
            </p>
            <ul>
              <li>
                <strong>Contextualized Similarity Predicts Behavior:</strong> In Qwen2.5-VL, contextual embeddings of hyponyms and their hypernyms were significantly more predictive of correct task performance (measured using odds ratio) than in its LM counterpart Qwen2.5.
              </li>
            </ul>
            <div class="content has-text-centered mt-3">
              <img src="static/images/contextualized_analysis_odds_ratio.png" alt="Odds ratios for contextual similarity predicting accuracy" style="max-width: 50%; height: auto;">
              <p class="is-size-7">Log odds ratios from logistic regression: contextualized similarity predicts accuracy more strongly in Qwen2.5-VL.</p>
            </div>
            <ul>
              <li>
                <strong>More Distinct Question Representations:</strong> PCA of question embeddings showed that VLMs more clearly separate taxonomic vs. non-taxonomic substitutions. A linear SVM trained on the top 2 PCs had lower error for VLMs (0.36) than for LMs (0.52), indicating clearer task-relevant distinctions.
              </li>
            </ul>
            <div class="content has-text-centered">
              <img src="static/images/Contextualized_Representational_analysis.png" alt="Contextualized Representational Analysis" width="75%" height="auto">
              <p class="is-size-7">Contextualized representational analysis on Qwen2.5-I (LM) and Qwen2.5-VL-I (VLM).</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>  
<!-- End Experiments & Key Findings -->

<!-- Why Vision Might Help -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Why Vision Might Help: The Role of Visual Similarity</h2>
          <div class="content has-text-justified">
            <p>
              To understand <em>why</em> VL training improves taxonomic reasoning in text-only settings, we explored whether visual similarity between related concepts helps models apply their knowledge more effectively.
            </p>
            <ul>
              <li>
                <strong>Visual Similarity predicts VLM Accuracy much better than it does LM accuracy:</strong> For each hypernym-hyponym pair, we computed visual similarity using embeddings from the VLM’s vision encoder on independent images from the <a href="https://things-initiative.org" target="_blank">THINGS</a> dataset. Higher similarity was associated with higher VLM conditional accuracy on corresponding taxonomic questions (<code>b = 0.52, SE = 0.19, p &lt; .01</code>). No such effect was found for LMs (<code>b = 0.23, SE = 0.17, p = 0.18</code>).
              </li>
              <li>
                <strong>Impact Varies by Concept:</strong> The strength of this effect varied across hypernyms. Concepts whose members were more visually cohesive (e.g., <em>band</em>, <em>stick</em>) showed stronger visual-similarity benefits, while broad or heterogeneous categories (e.g., <em>animal</em>, <em>structure</em>) showed weaker effects.
              </li>
            </ul>
            <div class="content has-text-centered">
              <img src="static/images/Hypernym_specific_random_effects.png" alt="Hypernym-specific random effects of image-similarity" width="100%" height="auto">
              <p class="is-size-7">Effect of visual similarity on VLM performance across hypernym categories. Bar color reflects visual cohesion (darker = higher cohesion).</p>
            </div>
            <p>
              These results suggest that VL training may help models exploit visual regularities in concept hierarchies, especially when categories have largely similar visual features.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>  
<!-- End Why Vision Might Help -->

<!-- Conclusion & Future Work -->
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Conclusion & Future Work</h2>
          <div class="content has-text-justified">
            <p>
              Our findings show that vision-and-language training doesn’t fundamentally change a model’s taxonomic knowledge. Instead, it improves how models <em>apply</em> that knowledge when solving tasks. Despite being evaluated on a purely text-based dataset, VLMs outperformed their LM counterparts across all metrics.
            </p>
            <p>
              Through a series of behavioral and representational analyses, we find that VLMs form stronger contextual links between related concepts and represent taxonomic distinctions more distinctly in downstream tasks. This advantage appears linked to the visual similarity and cohesion among concepts in the taxonomic hierarchy.
            </p>
            <p>
              <strong>Looking ahead:</strong> our analyses are correlational in nature. A key direction for future work is establishing <em>causal</em> links between VL training and task-specific deployment behavior, potentially by analyzing training data, probing internal objectives, or manipulating visual features during pretraining. There is also room to explore whether VLMs encode non-linear taxonomic distinctions that aren’t captured by the linear separability analyses used here.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>  
<!-- End Conclusion & Future Work -->

<!-- Computational Resources -->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br><br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Computational Resources</h2>
      </div>
      <h2 class="subtitle has-text-centered">
        Our extensive analyses, including model inference and various behavioral and representational studies, were efficiently performed on NVIDIA A40, L40, and RTX6000 Ada GPUs. Most core evaluations completed within a few hours on single GPUs, ensuring these techniques are broadly accessible for future research.
      </h2>
    </div>
  </div>
</section> -->
<!-- End Computational Resources -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
        <pre><code>TBD.</code></pre>
  </div>
</section>

<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>